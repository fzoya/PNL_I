{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yeJGnCYxuF"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Modelo de lenguaje con tokenización por caracteres\n",
        "\n",
        "- Alumno: FEDERICO MARTIN ZOYA - a1828"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5PEwGzZA9-"
      },
      "source": [
        "### Consigna\n",
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "### Sugerencias\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y-QdFbHZYj7C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-14 14:26:42.870594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-06-14 14:26:42.886964: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-06-14 14:26:42.891879: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-14 14:26:42.904038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-06-14 14:26:43.662282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvXlEKQZdqx"
      },
      "source": [
        "### Datos\n",
        "Utilizaremos como dataset canciones de bandas de habla inglés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7amy6uUaBLVD"
      },
      "outputs": [],
      "source": [
        "# descargar de textos.info\n",
        "import urllib.request\n",
        "\n",
        "# Para leer y parsear el texto en HTML de wikipedia\n",
        "import bs4 as bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6v_ickFwBJTy"
      },
      "outputs": [],
      "source": [
        "# ####################################################################################################\n",
        "# Para este trabajo se opta por el texto \"El Príncipe\" de Nicolas Maquiavelo.\n",
        "# ####################################################################################################\n",
        "raw_html = urllib.request.urlopen('https://www.textos.info/nicolas-maquiavelo/el-principe/ebook')\n",
        "raw_html = raw_html.read()\n",
        "\n",
        "# Se pre-procesa el nuevo corpus de la misma manera que el texto original.\n",
        "# Parsear artículo, 'lxml' es el parser a utilizar\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "# Se trabaja con el HTML correspondiente al texto servido por el sitio web\n",
        "# Encontrar todos los párrafos del HTML (bajo el tag <p>)\n",
        "# y tenerlos disponible como lista\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:\n",
        "    article_text += para.text + ' '\n",
        "\n",
        "# ####################################################################################################\n",
        "# Una vez recuperado todo el texto de la página web se define el corpus en una variable local\n",
        "# ####################################################################################################\n",
        "# pasar todo el texto a minúscula\n",
        "article_text = article_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "WBE0sSYuB-E6",
        "outputId": "d922d3d4-e8db-45c6-820b-2c0696ca6e08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' nicolas maquiavelo,\\r\\nciudadano y secretario de florencia,\\r\\nal\\r\\nmagnífico señor\\r\\nlorenzo de medicis,\\r\\nduque de urbino, señor de pésaro,\\r\\netc. etc. los que desean captarse la voluntad de un príncipe suelen\\r\\nofrecerle presentes de raro mérito, o aquellas cosas que son\\r\\nconocidamente de su agrado: unos le presentan armas o caballos, otros\\r\\ntelas de oro, piedras preciosas, alhajas, en fin, dignas de su grandeza.\\r\\ndeseando yo, pues, ofreceros una prueba de mi adhesión y respetuosa\\r\\nobediencia, he encontrado que la alhaja de más valor, y tal vez la única\\r\\nque poseo, es el conocimiento de lo que han hecho los grandes hombres;\\r\\nconocimiento que he adquirido con una larga experiencia de la política\\r\\nmoderna, y una lectura continua de la que seguían los antiguos. de todo\\r\\nesto, meditado y examinado con detención escrupulosa, he formado un\\r\\npequeño volumen, que os envío, pues, aunque creo que mi obra es indigna\\r\\nde tamaño honor, sin embargo, confío en que será acogida con\\r\\nbenevolencia, considera'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# En la variable article_text se encuentra el texto de todo el libro forzado a minúsculas\n",
        "article_text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "47MCYyhYFvqL",
        "outputId": "0f1bbc2b-aa23-4cc9-c180-5316b53afc42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' nicolas maquiavelo, ciudadano y secretario de florencia, al magnífico señor lorenzo de medicis, duque de urbino, señor de pésaro, etc. etc. los que desean captarse la voluntad de un príncipe suelen ofrecerle presentes de raro mérito, o aquellas cosas que son conocidamente de su agrado: unos le presentan armas o caballos, otros telas de oro, piedras preciosas, alhajas, en fin, dignas de su grandeza. deseando yo, pues, ofreceros una prueba de mi adhesión y respetuosa obediencia, he encontrado que la alhaja de más valor, y tal vez la única que poseo, es el conocimiento de lo que han hecho los grandes hombres; conocimiento que he adquirido con una larga experiencia de la política moderna, y una lectura continua de la que seguían los antiguos. de todo esto, meditado y examinado con detención escrupulosa, he formado un pequeño volumen, que os envío, pues, aunque creo que mi obra es indigna de tamaño honor, sin embargo, confío en que será acogida con benevolencia, considerando que no puedo o'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ####################################################################################################\n",
        "# Se eliminan caracteres ocultos que no aportan información relevante de la semántica del texto\n",
        "# ####################################################################################################\n",
        "article_text = article_text.replace('\\n',' ')\n",
        "article_text = article_text.replace('\\r',' ')\n",
        "article_text = article_text.replace('\\t',' ')\n",
        "article_text = article_text.replace('  ',' ')\n",
        "article_text[:1000]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP1JdiOIKQWi"
      },
      "source": [
        "### Elegir el tamaño del contexto\n",
        "\n",
        "En este caso, como el modelo de lenguaje es por caracteres, todo un gran corpus\n",
        "de texto puede ser considerado un documento en sí mismo y el tamaño de contexto\n",
        "puede ser elegido con más libertad en comparación a un modelo de lenguaje tokenizado por palabras y dividido en documentos más acotados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wumBNwdjJM3j"
      },
      "outputs": [],
      "source": [
        "# seleccionamos diferentes tamaños de contexto con la finalidad de efectuar pruebas\n",
        "#max_context_size = 100\n",
        "max_context_size = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m5FeTaGvbDbw"
      },
      "outputs": [],
      "source": [
        "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
        "from tensorflow.keras.utils import pad_sequences # se utilizará para padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "573Cg5n7VhWw",
        "outputId": "439746db-6d02-491b-bded-b867976312ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'8', '(', ',', 's', 'a', 't', 'ñ', 'z', 'j', 'v', '»', 'l', 'u', 'i', 'q', 'á', 'º', 'f', ')', '—', '°', 'y', 'h', '0', 'e', '.', 'ú', 'c', 'g', '3', '-', 'é', 'o', 'd', \"'\", '7', '1', '2', 'b', '4', ':', 'ü', 'í', '5', '/', '?', '6', '«', 'n', 'r', ';', ' ', 'p', 'ó', 'm', '¿', 'x'}\n"
          ]
        }
      ],
      "source": [
        "# Al tratarse de tokenización por caracteres, el vocabulario se reduce al conjunto único de caracteres que existe en todo el texto.\n",
        "chars_vocab = set(article_text)\n",
        "print(chars_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwTK6xgLJd8q",
        "outputId": "034ebb78-ee3d-4306-ed9c-6027f3821e9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# la longitud de vocabulario de caracteres es:\n",
        "len(chars_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2W0AeQjXV1Ou"
      },
      "outputs": [],
      "source": [
        "# Construimos los dicionarios que asignan índices a caracteres y viceversa.\n",
        "# El diccionario `char2idx` servirá como tokenizador.\n",
        "char2idx = {k: v for v,k in enumerate(chars_vocab)}\n",
        "idx2char = {v: k for k,v in char2idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oIUjVU0LB0r"
      },
      "source": [
        "###  Tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "h07G3srdJppo"
      },
      "outputs": [],
      "source": [
        "# ####################################################################################################\n",
        "# Se tokeniza el vocabulario a partir de los índices generados por caracter. Tal vez se podría tomar\n",
        "# directamente el código ASCII asociado.\n",
        "# ####################################################################################################\n",
        "# tokenizamos el texto completo\n",
        "tokenized_text = [char2idx[ch] for ch in article_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwGVSKOiJ5bj",
        "outputId": "e016c3e4-8a9d-4978-c06a-0691cb68adf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[51,\n",
              " 48,\n",
              " 13,\n",
              " 27,\n",
              " 32,\n",
              " 11,\n",
              " 4,\n",
              " 3,\n",
              " 51,\n",
              " 54,\n",
              " 4,\n",
              " 14,\n",
              " 12,\n",
              " 13,\n",
              " 4,\n",
              " 9,\n",
              " 24,\n",
              " 11,\n",
              " 32,\n",
              " 2,\n",
              " 51,\n",
              " 27,\n",
              " 13,\n",
              " 12,\n",
              " 33,\n",
              " 4,\n",
              " 33,\n",
              " 4,\n",
              " 48,\n",
              " 32,\n",
              " 51,\n",
              " 21,\n",
              " 51,\n",
              " 3,\n",
              " 24,\n",
              " 27,\n",
              " 49,\n",
              " 24,\n",
              " 5,\n",
              " 4,\n",
              " 49,\n",
              " 13,\n",
              " 32,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 17,\n",
              " 11,\n",
              " 32,\n",
              " 49,\n",
              " 24,\n",
              " 48,\n",
              " 27,\n",
              " 13,\n",
              " 4,\n",
              " 2,\n",
              " 51,\n",
              " 4,\n",
              " 11,\n",
              " 51,\n",
              " 54,\n",
              " 4,\n",
              " 28,\n",
              " 48,\n",
              " 42,\n",
              " 17,\n",
              " 13,\n",
              " 27,\n",
              " 32,\n",
              " 51,\n",
              " 3,\n",
              " 24,\n",
              " 6,\n",
              " 32,\n",
              " 49,\n",
              " 51,\n",
              " 11,\n",
              " 32,\n",
              " 49,\n",
              " 24,\n",
              " 48,\n",
              " 7,\n",
              " 32,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 54,\n",
              " 24,\n",
              " 33,\n",
              " 13,\n",
              " 27,\n",
              " 13,\n",
              " 3,\n",
              " 2,\n",
              " 51,\n",
              " 33,\n",
              " 12,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 12,\n",
              " 49,\n",
              " 38,\n",
              " 13,\n",
              " 48,\n",
              " 32,\n",
              " 2,\n",
              " 51,\n",
              " 3,\n",
              " 24,\n",
              " 6,\n",
              " 32,\n",
              " 49,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 52,\n",
              " 31,\n",
              " 3,\n",
              " 4,\n",
              " 49,\n",
              " 32,\n",
              " 2,\n",
              " 51,\n",
              " 24,\n",
              " 5,\n",
              " 27,\n",
              " 25,\n",
              " 51,\n",
              " 24,\n",
              " 5,\n",
              " 27,\n",
              " 25,\n",
              " 51,\n",
              " 11,\n",
              " 32,\n",
              " 3,\n",
              " 51,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 3,\n",
              " 24,\n",
              " 4,\n",
              " 48,\n",
              " 51,\n",
              " 27,\n",
              " 4,\n",
              " 52,\n",
              " 5,\n",
              " 4,\n",
              " 49,\n",
              " 3,\n",
              " 24,\n",
              " 51,\n",
              " 11,\n",
              " 4,\n",
              " 51,\n",
              " 9,\n",
              " 32,\n",
              " 11,\n",
              " 12,\n",
              " 48,\n",
              " 5,\n",
              " 4,\n",
              " 33,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 12,\n",
              " 48,\n",
              " 51,\n",
              " 52,\n",
              " 49,\n",
              " 42,\n",
              " 48,\n",
              " 27,\n",
              " 13,\n",
              " 52,\n",
              " 24,\n",
              " 51,\n",
              " 3,\n",
              " 12,\n",
              " 24,\n",
              " 11,\n",
              " 24,\n",
              " 48,\n",
              " 51,\n",
              " 32,\n",
              " 17,\n",
              " 49,\n",
              " 24,\n",
              " 27,\n",
              " 24,\n",
              " 49,\n",
              " 11,\n",
              " 24,\n",
              " 51,\n",
              " 52,\n",
              " 49,\n",
              " 24,\n",
              " 3,\n",
              " 24,\n",
              " 48,\n",
              " 5,\n",
              " 24,\n",
              " 3,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 49,\n",
              " 4,\n",
              " 49,\n",
              " 32,\n",
              " 51,\n",
              " 54,\n",
              " 31,\n",
              " 49,\n",
              " 13,\n",
              " 5,\n",
              " 32,\n",
              " 2,\n",
              " 51,\n",
              " 32,\n",
              " 51,\n",
              " 4,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 11,\n",
              " 11,\n",
              " 4,\n",
              " 3,\n",
              " 51,\n",
              " 27,\n",
              " 32,\n",
              " 3,\n",
              " 4,\n",
              " 3,\n",
              " 51,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 3,\n",
              " 32,\n",
              " 48,\n",
              " 51,\n",
              " 27,\n",
              " 32,\n",
              " 48,\n",
              " 32,\n",
              " 27,\n",
              " 13,\n",
              " 33,\n",
              " 4,\n",
              " 54,\n",
              " 24,\n",
              " 48,\n",
              " 5,\n",
              " 24,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 3,\n",
              " 12,\n",
              " 51,\n",
              " 4,\n",
              " 28,\n",
              " 49,\n",
              " 4,\n",
              " 33,\n",
              " 32,\n",
              " 40,\n",
              " 51,\n",
              " 12,\n",
              " 48,\n",
              " 32,\n",
              " 3,\n",
              " 51,\n",
              " 11,\n",
              " 24,\n",
              " 51,\n",
              " 52,\n",
              " 49,\n",
              " 24,\n",
              " 3,\n",
              " 24,\n",
              " 48,\n",
              " 5,\n",
              " 4,\n",
              " 48,\n",
              " 51,\n",
              " 4,\n",
              " 49,\n",
              " 54,\n",
              " 4,\n",
              " 3,\n",
              " 51,\n",
              " 32,\n",
              " 51,\n",
              " 27,\n",
              " 4,\n",
              " 38,\n",
              " 4,\n",
              " 11,\n",
              " 11,\n",
              " 32,\n",
              " 3,\n",
              " 2,\n",
              " 51,\n",
              " 32,\n",
              " 5,\n",
              " 49,\n",
              " 32,\n",
              " 3,\n",
              " 51,\n",
              " 5,\n",
              " 24,\n",
              " 11,\n",
              " 4,\n",
              " 3,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 32,\n",
              " 49,\n",
              " 32,\n",
              " 2,\n",
              " 51,\n",
              " 52,\n",
              " 13,\n",
              " 24,\n",
              " 33,\n",
              " 49,\n",
              " 4,\n",
              " 3,\n",
              " 51,\n",
              " 52,\n",
              " 49,\n",
              " 24,\n",
              " 27,\n",
              " 13,\n",
              " 32,\n",
              " 3,\n",
              " 4,\n",
              " 3,\n",
              " 2,\n",
              " 51,\n",
              " 4,\n",
              " 11,\n",
              " 22,\n",
              " 4,\n",
              " 8,\n",
              " 4,\n",
              " 3,\n",
              " 2,\n",
              " 51,\n",
              " 24,\n",
              " 48,\n",
              " 51,\n",
              " 17,\n",
              " 13,\n",
              " 48,\n",
              " 2,\n",
              " 51,\n",
              " 33,\n",
              " 13,\n",
              " 28,\n",
              " 48,\n",
              " 4,\n",
              " 3,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 3,\n",
              " 12,\n",
              " 51,\n",
              " 28,\n",
              " 49,\n",
              " 4,\n",
              " 48,\n",
              " 33,\n",
              " 24,\n",
              " 7,\n",
              " 4,\n",
              " 25,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 3,\n",
              " 24,\n",
              " 4,\n",
              " 48,\n",
              " 33,\n",
              " 32,\n",
              " 51,\n",
              " 21,\n",
              " 32,\n",
              " 2,\n",
              " 51,\n",
              " 52,\n",
              " 12,\n",
              " 24,\n",
              " 3,\n",
              " 2,\n",
              " 51,\n",
              " 32,\n",
              " 17,\n",
              " 49,\n",
              " 24,\n",
              " 27,\n",
              " 24,\n",
              " 49,\n",
              " 32,\n",
              " 3,\n",
              " 51,\n",
              " 12,\n",
              " 48,\n",
              " 4,\n",
              " 51,\n",
              " 52,\n",
              " 49,\n",
              " 12,\n",
              " 24,\n",
              " 38,\n",
              " 4,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 54,\n",
              " 13,\n",
              " 51,\n",
              " 4,\n",
              " 33,\n",
              " 22,\n",
              " 24,\n",
              " 3,\n",
              " 13,\n",
              " 53,\n",
              " 48,\n",
              " 51,\n",
              " 21,\n",
              " 51,\n",
              " 49,\n",
              " 24,\n",
              " 3,\n",
              " 52,\n",
              " 24,\n",
              " 5,\n",
              " 12,\n",
              " 32,\n",
              " 3,\n",
              " 4,\n",
              " 51,\n",
              " 32,\n",
              " 38,\n",
              " 24,\n",
              " 33,\n",
              " 13,\n",
              " 24,\n",
              " 48,\n",
              " 27,\n",
              " 13,\n",
              " 4,\n",
              " 2,\n",
              " 51,\n",
              " 22,\n",
              " 24,\n",
              " 51,\n",
              " 24,\n",
              " 48,\n",
              " 27,\n",
              " 32,\n",
              " 48,\n",
              " 5,\n",
              " 49,\n",
              " 4,\n",
              " 33,\n",
              " 32,\n",
              " 51,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 11,\n",
              " 4,\n",
              " 51,\n",
              " 4,\n",
              " 11,\n",
              " 22,\n",
              " 4,\n",
              " 8,\n",
              " 4,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 54,\n",
              " 15,\n",
              " 3,\n",
              " 51,\n",
              " 9,\n",
              " 4,\n",
              " 11,\n",
              " 32,\n",
              " 49,\n",
              " 2,\n",
              " 51,\n",
              " 21,\n",
              " 51,\n",
              " 5,\n",
              " 4,\n",
              " 11,\n",
              " 51,\n",
              " 9,\n",
              " 24,\n",
              " 7,\n",
              " 51,\n",
              " 11,\n",
              " 4,\n",
              " 51,\n",
              " 26,\n",
              " 48,\n",
              " 13,\n",
              " 27,\n",
              " 4,\n",
              " 51,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 52,\n",
              " 32,\n",
              " 3,\n",
              " 24,\n",
              " 32,\n",
              " 2,\n",
              " 51,\n",
              " 24,\n",
              " 3,\n",
              " 51,\n",
              " 24,\n",
              " 11,\n",
              " 51,\n",
              " 27,\n",
              " 32,\n",
              " 48,\n",
              " 32,\n",
              " 27,\n",
              " 13,\n",
              " 54,\n",
              " 13,\n",
              " 24,\n",
              " 48,\n",
              " 5,\n",
              " 32,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 11,\n",
              " 32,\n",
              " 51,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 22,\n",
              " 4,\n",
              " 48,\n",
              " 51,\n",
              " 22,\n",
              " 24,\n",
              " 27,\n",
              " 22,\n",
              " 32,\n",
              " 51,\n",
              " 11,\n",
              " 32,\n",
              " 3,\n",
              " 51,\n",
              " 28,\n",
              " 49,\n",
              " 4,\n",
              " 48,\n",
              " 33,\n",
              " 24,\n",
              " 3,\n",
              " 51,\n",
              " 22,\n",
              " 32,\n",
              " 54,\n",
              " 38,\n",
              " 49,\n",
              " 24,\n",
              " 3,\n",
              " 50,\n",
              " 51,\n",
              " 27,\n",
              " 32,\n",
              " 48,\n",
              " 32,\n",
              " 27,\n",
              " 13,\n",
              " 54,\n",
              " 13,\n",
              " 24,\n",
              " 48,\n",
              " 5,\n",
              " 32,\n",
              " 51,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 22,\n",
              " 24,\n",
              " 51,\n",
              " 4,\n",
              " 33,\n",
              " 14,\n",
              " 12,\n",
              " 13,\n",
              " 49,\n",
              " 13,\n",
              " 33,\n",
              " 32,\n",
              " 51,\n",
              " 27,\n",
              " 32,\n",
              " 48,\n",
              " 51,\n",
              " 12,\n",
              " 48,\n",
              " 4,\n",
              " 51,\n",
              " 11,\n",
              " 4,\n",
              " 49,\n",
              " 28,\n",
              " 4,\n",
              " 51,\n",
              " 24,\n",
              " 56,\n",
              " 52,\n",
              " 24,\n",
              " 49,\n",
              " 13,\n",
              " 24,\n",
              " 48,\n",
              " 27,\n",
              " 13,\n",
              " 4,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 11,\n",
              " 4,\n",
              " 51,\n",
              " 52,\n",
              " 32,\n",
              " 11,\n",
              " 42,\n",
              " 5,\n",
              " 13,\n",
              " 27,\n",
              " 4,\n",
              " 51,\n",
              " 54,\n",
              " 32,\n",
              " 33,\n",
              " 24,\n",
              " 49,\n",
              " 48,\n",
              " 4,\n",
              " 2,\n",
              " 51,\n",
              " 21,\n",
              " 51,\n",
              " 12,\n",
              " 48,\n",
              " 4,\n",
              " 51,\n",
              " 11,\n",
              " 24,\n",
              " 27,\n",
              " 5,\n",
              " 12,\n",
              " 49,\n",
              " 4,\n",
              " 51,\n",
              " 27,\n",
              " 32,\n",
              " 48,\n",
              " 5,\n",
              " 13,\n",
              " 48,\n",
              " 12,\n",
              " 4,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 11,\n",
              " 4,\n",
              " 51,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 3,\n",
              " 24,\n",
              " 28,\n",
              " 12,\n",
              " 42,\n",
              " 4,\n",
              " 48,\n",
              " 51,\n",
              " 11,\n",
              " 32,\n",
              " 3,\n",
              " 51,\n",
              " 4,\n",
              " 48,\n",
              " 5,\n",
              " 13,\n",
              " 28,\n",
              " 12,\n",
              " 32,\n",
              " 3,\n",
              " 25,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 5,\n",
              " 32,\n",
              " 33,\n",
              " 32,\n",
              " 51,\n",
              " 24,\n",
              " 3,\n",
              " 5,\n",
              " 32,\n",
              " 2,\n",
              " 51,\n",
              " 54,\n",
              " 24,\n",
              " 33,\n",
              " 13,\n",
              " 5,\n",
              " 4,\n",
              " 33,\n",
              " 32,\n",
              " 51,\n",
              " 21,\n",
              " 51,\n",
              " 24,\n",
              " 56,\n",
              " 4,\n",
              " 54,\n",
              " 13,\n",
              " 48,\n",
              " 4,\n",
              " 33,\n",
              " 32,\n",
              " 51,\n",
              " 27,\n",
              " 32,\n",
              " 48,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 5,\n",
              " 24,\n",
              " 48,\n",
              " 27,\n",
              " 13,\n",
              " 53,\n",
              " 48,\n",
              " 51,\n",
              " 24,\n",
              " 3,\n",
              " 27,\n",
              " 49,\n",
              " 12,\n",
              " 52,\n",
              " 12,\n",
              " 11,\n",
              " 32,\n",
              " 3,\n",
              " 4,\n",
              " 2,\n",
              " 51,\n",
              " 22,\n",
              " 24,\n",
              " 51,\n",
              " 17,\n",
              " 32,\n",
              " 49,\n",
              " 54,\n",
              " 4,\n",
              " 33,\n",
              " 32,\n",
              " 51,\n",
              " 12,\n",
              " 48,\n",
              " 51,\n",
              " 52,\n",
              " 24,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 6,\n",
              " 32,\n",
              " 51,\n",
              " 9,\n",
              " 32,\n",
              " 11,\n",
              " 12,\n",
              " 54,\n",
              " 24,\n",
              " 48,\n",
              " 2,\n",
              " 51,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 32,\n",
              " 3,\n",
              " 51,\n",
              " 24,\n",
              " 48,\n",
              " 9,\n",
              " 42,\n",
              " 32,\n",
              " 2,\n",
              " 51,\n",
              " 52,\n",
              " 12,\n",
              " 24,\n",
              " 3,\n",
              " 2,\n",
              " 51,\n",
              " 4,\n",
              " 12,\n",
              " 48,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 27,\n",
              " 49,\n",
              " 24,\n",
              " 32,\n",
              " 51,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 54,\n",
              " 13,\n",
              " 51,\n",
              " 32,\n",
              " 38,\n",
              " 49,\n",
              " 4,\n",
              " 51,\n",
              " 24,\n",
              " 3,\n",
              " 51,\n",
              " 13,\n",
              " 48,\n",
              " 33,\n",
              " 13,\n",
              " 28,\n",
              " 48,\n",
              " 4,\n",
              " 51,\n",
              " 33,\n",
              " 24,\n",
              " 51,\n",
              " 5,\n",
              " 4,\n",
              " 54,\n",
              " 4,\n",
              " 6,\n",
              " 32,\n",
              " 51,\n",
              " 22,\n",
              " 32,\n",
              " 48,\n",
              " 32,\n",
              " 49,\n",
              " 2,\n",
              " 51,\n",
              " 3,\n",
              " 13,\n",
              " 48,\n",
              " 51,\n",
              " 24,\n",
              " 54,\n",
              " 38,\n",
              " 4,\n",
              " 49,\n",
              " 28,\n",
              " 32,\n",
              " 2,\n",
              " 51,\n",
              " 27,\n",
              " 32,\n",
              " 48,\n",
              " 17,\n",
              " 42,\n",
              " 32,\n",
              " 51,\n",
              " 24,\n",
              " 48,\n",
              " 51,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 3,\n",
              " 24,\n",
              " 49,\n",
              " 15,\n",
              " 51,\n",
              " 4,\n",
              " 27,\n",
              " 32,\n",
              " 28,\n",
              " 13,\n",
              " 33,\n",
              " 4,\n",
              " 51,\n",
              " 27,\n",
              " 32,\n",
              " 48,\n",
              " 51,\n",
              " 38,\n",
              " 24,\n",
              " 48,\n",
              " 24,\n",
              " 9,\n",
              " 32,\n",
              " 11,\n",
              " 24,\n",
              " 48,\n",
              " 27,\n",
              " 13,\n",
              " 4,\n",
              " 2,\n",
              " 51,\n",
              " 27,\n",
              " 32,\n",
              " 48,\n",
              " 3,\n",
              " 13,\n",
              " 33,\n",
              " 24,\n",
              " 49,\n",
              " 4,\n",
              " 48,\n",
              " 33,\n",
              " 32,\n",
              " 51,\n",
              " 14,\n",
              " 12,\n",
              " 24,\n",
              " 51,\n",
              " 48,\n",
              " 32,\n",
              " 51,\n",
              " 52,\n",
              " 12,\n",
              " 24,\n",
              " 33,\n",
              " 32,\n",
              " 51,\n",
              " 32]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_text[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpYcaypKcI9"
      },
      "source": [
        "### Organizando y estructurando el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WSSmg9jtKP0T"
      },
      "outputs": [],
      "source": [
        "# ####################################################################################################\n",
        "# Conformación de los set de trabajo:\n",
        "# separaremos el dataset entre entrenamiento y validación en 10 y 20% para probar mejoras.\n",
        "# `p_val` será la proporción del corpus que se reservará para validación\n",
        "# `num_val` es la cantidad de secuencias de tamaño `max_context_size` que se usará en validación\n",
        "# ####################################################################################################\n",
        "#p_val = 0.1\n",
        "p_val = 0.2\n",
        "num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b7dCpGrdKll0"
      },
      "outputs": [],
      "source": [
        "# separamos la porción de texto utilizada en entrenamiento de la de validación.\n",
        "train_text = tokenized_text[:-num_val*max_context_size]\n",
        "val_text = tokenized_text[-num_val*max_context_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NmxQdxl8LRCg"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_val = [val_text[init*max_context_size:init*(max_context_size+1)] for init in range(num_val)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_gyFT9koLqDm"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_train = [train_text[init:init+max_context_size] for init in range(len(train_text)-max_context_size+1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oVNqmmLRodT0"
      },
      "outputs": [],
      "source": [
        "X = np.array(tokenized_sentences_train[:-1])\n",
        "y = np.array(tokenized_sentences_train[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vken7O4ETsAJ"
      },
      "source": [
        "Nótese que estamos estructurando el problema de aprendizaje como *many-to-many*:\n",
        "\n",
        "Entrada: secuencia de tokens [$x_0$, $x_1$, ..., $x_N$]\n",
        "\n",
        "Target: secuencia de tokens [$x_1$, $x_2$, ..., $x_{N+1}$]\n",
        "\n",
        "De manera que la red tiene que aprender que su salida deben ser los tokens desplazados en una posición y un nuevo token predicho (el N+1).\n",
        "\n",
        "La ventaja de estructurar el aprendizaje de esta manera es que para cada token de target se propaga una señal de gradiente por el grafo de cómputo recurrente, que es mejor que estructurar el problema como *many-to-one* en donde sólo una señal de gradiente se propaga."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iPTx-UJl6r"
      },
      "source": [
        "En este punto tenemos en la variable `tokenized_sentences` los versos tokenizados. Vamos a quedarnos con un conjunto de validación que utilizaremos para medir la calidad de la generación de secuencias con la métrica de Perplejidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFAyA4zCWE-5",
        "outputId": "87aa9d2d-0a9d-4903-b369-a78b660b47e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(143272, 200)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcKRl70HFTzG",
        "outputId": "85b65ac1-eaee-4762-bed9-ecceddd570e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([51, 48, 13, 27, 32, 11,  4,  3, 51, 54])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVpLCKSZFXZO",
        "outputId": "dca7d3f0-afdf-4353-fa9a-5aec60405d28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([48, 13, 27, 32, 11,  4,  3, 51, 54,  4])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[0,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wOFCR-KqbW1N"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(chars_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnnjdAQ5UAEJ"
      },
      "source": [
        "# Definiendo el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rkMCZvmhrQz4"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, Dense, GRU\n",
        "from keras.models import Model, Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgz7VKwTUbj6"
      },
      "source": [
        "El modelo que se propone como ejemplo consume los índices de los tokens y los transforma en vectores OHE (en este caso no entrenamos una capa de embedding para caracteres). Esa transformación se logra combinando las capas `CategoryEncoding` que transforma a índices a vectores OHE y `TimeDistributed` que aplica la capa a lo largo de la dimensión \"temporal\" de la secuencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Zd2OkfQYs2Q7",
        "outputId": "fec1c085-7796-4887-f863-c5b1f434d09c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/federico/ClasesIA/CEIA-IA/lib/python3.12/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1749922012.776252   21918 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2025-06-14 14:26:52.843083: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">155,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">11,457</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)      │       \u001b[38;5;34m155,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57\u001b[0m)       │        \u001b[38;5;34m11,457\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">166,857</span> (651.79 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m166,857\u001b[0m (651.79 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">166,857</span> (651.79 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m166,857\u001b[0m (651.79 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode = \"one_hot\"),input_shape=(None,1)))\n",
        "#model.add(SimpleRNN(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1 )) # SE REEMPLAZA POR LSTM MANTENIENDO PARAMETROS\n",
        "#model.add(LSTM(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "model.add(GRU(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmJWNyxQwfCE"
      },
      "source": [
        "\n",
        "### Definir el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWK3z85sQfUe"
      },
      "source": [
        "Dado que por el momento no hay implementaciones adecuadas de la perplejidad que puedan operar en tiempo de entrenamiento, armaremos un Callback *ad-hoc* que la calcule en cada epoch.\n",
        "\n",
        "**Nota**: un Callback es una rutina gatillada por algún evento, son muy útiles para relevar datos en diferentes momentos del desarrollo del modelo. En este caso queremos hacer un cálculo cada vez que termina una epoch de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zUHX3r5JD-MG"
      },
      "outputs": [],
      "source": [
        "class PplCallback(keras.callbacks.Callback):\n",
        "\n",
        "    '''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
        "    si la perplejidad no mejora después de `patience` epochs.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data, history_ppl,patience=5):\n",
        "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "      # mediremos la perplejidad\n",
        "      self.val_data = val_data\n",
        "\n",
        "      self.target = []\n",
        "      self.padded = []\n",
        "\n",
        "      count = 0\n",
        "      self.info = []\n",
        "      self.min_score = np.inf\n",
        "      self.patience_counter = 0\n",
        "      self.patience = patience\n",
        "\n",
        "      # nos movemos en todas las secuencias de los datos de validación\n",
        "      for seq in self.val_data:\n",
        "\n",
        "        len_seq = len(seq)\n",
        "        # armamos todas las subsecuencias\n",
        "        subseq = [seq[:i] for i in range(1,len_seq)]\n",
        "        self.target.extend([seq[i] for i in range(1,len_seq)])\n",
        "\n",
        "        if len(subseq)!=0:\n",
        "\n",
        "          self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "\n",
        "          self.info.append((count,count+len_seq))\n",
        "          count += len_seq\n",
        "\n",
        "      self.padded = np.vstack(self.padded)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        "        scores = []\n",
        "\n",
        "        predictions = self.model.predict(self.padded,verbose=0)\n",
        "\n",
        "        # para cada secuencia de validación\n",
        "        for start,end in self.info:\n",
        "\n",
        "          # en `probs` iremos guardando las probabilidades de los términos target\n",
        "          probs = [predictions[idx_seq,-1,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
        "\n",
        "          # calculamos la perplejidad por medio de logaritmos\n",
        "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        current_score = np.mean(scores)\n",
        "        history_ppl.append(current_score)\n",
        "        print(f'\\n mean perplexity: {current_score} \\n')\n",
        "\n",
        "        # chequeamos si tenemos que detener el entrenamiento\n",
        "        if current_score < self.min_score:\n",
        "          self.min_score = current_score\n",
        "          self.model.save(\"my_model.keras\")\n",
        "          print(\"Saved new model!\")\n",
        "          self.patience_counter = 0\n",
        "        else:\n",
        "          self.patience_counter += 1\n",
        "          if self.patience_counter == self.patience:\n",
        "            print(\"Stopping training...\")\n",
        "            self.model.stop_training = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBZIwR0gruA"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "oQq1PHDkxDvN",
        "outputId": "e7c076f6-c81b-4909-f70e-1d9bbca4f423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - loss: 2.6329\n",
            " mean perplexity: 7.858747018255527 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 530ms/step - loss: 2.6323\n",
            "Epoch 2/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507ms/step - loss: 1.9763\n",
            " mean perplexity: 6.418106202871309 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 538ms/step - loss: 1.9762\n",
            "Epoch 3/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477ms/step - loss: 1.7637\n",
            " mean perplexity: 5.694019509968508 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 509ms/step - loss: 1.7636\n",
            "Epoch 4/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - loss: 1.6137\n",
            " mean perplexity: 5.210239934485326 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 523ms/step - loss: 1.6137\n",
            "Epoch 5/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509ms/step - loss: 1.5057\n",
            " mean perplexity: 4.963050775098247 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 541ms/step - loss: 1.5057\n",
            "Epoch 6/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508ms/step - loss: 1.4323\n",
            " mean perplexity: 4.838101093442378 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 541ms/step - loss: 1.4322\n",
            "Epoch 7/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513ms/step - loss: 1.3779\n",
            " mean perplexity: 4.770677710275556 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 546ms/step - loss: 1.3779\n",
            "Epoch 8/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508ms/step - loss: 1.3371\n",
            " mean perplexity: 4.739561779514466 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 541ms/step - loss: 1.3371\n",
            "Epoch 9/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513ms/step - loss: 1.3030\n",
            " mean perplexity: 4.715075518920977 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 546ms/step - loss: 1.3030\n",
            "Epoch 10/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514ms/step - loss: 1.2769\n",
            " mean perplexity: 4.697939912951634 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 547ms/step - loss: 1.2769\n",
            "Epoch 11/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - loss: 1.2532\n",
            " mean perplexity: 4.72709004287152 \n",
            "\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 572ms/step - loss: 1.2532\n",
            "Epoch 12/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502ms/step - loss: 1.2355\n",
            " mean perplexity: 4.722547173320403 \n",
            "\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 534ms/step - loss: 1.2355\n",
            "Epoch 13/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493ms/step - loss: 1.2187\n",
            " mean perplexity: 4.741214068898301 \n",
            "\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 525ms/step - loss: 1.2186\n",
            "Epoch 14/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476ms/step - loss: 1.2035\n",
            " mean perplexity: 4.805915843050873 \n",
            "\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 507ms/step - loss: 1.2035\n",
            "Epoch 15/20\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - loss: 1.1918\n",
            " mean perplexity: 4.819241718605429 \n",
            "\n",
            "Stopping training...\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 520ms/step - loss: 1.1918\n"
          ]
        }
      ],
      "source": [
        "# fiteamos, nótese el agregado del callback con su inicialización. El batch_size lo podemos seleccionar a mano\n",
        "# en general, lo mejor es escoger el batch más grande posible que minimice el tiempo de cada época.\n",
        "# En la variable `history_ppl` se guardarán los valores de perplejidad para cada época.\n",
        "history_ppl = []\n",
        "hist = model.fit(X, y, epochs=20, callbacks=[PplCallback(tokenized_sentences_val,history_ppl)], batch_size=255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K30JHB3Dv-mx"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(history_ppl) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history_ppl)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Rhy5hZN38qfO"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "# Cargamos el mejor modelo guardado del entrenamiento para hacer inferencia\n",
        "model = keras.models.load_model('my_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN6Fg_BsxJe6"
      },
      "source": [
        "\n",
        "### Predicción del próximo caracter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "IBvKHFPmzpy2"
      },
      "outputs": [],
      "source": [
        "# Se puede usar gradio para probar el modelo\n",
        "# Gradio es una herramienta muy útil para crear interfaces para ensayar modelos\n",
        "# https://gradio.app/\n",
        "\n",
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HNyBykvhzs7-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def model_response(human_text):\n",
        "\n",
        "    # Encodeamos\n",
        "    encoded = [char2idx[ch] for ch in human_text.lower() ]\n",
        "    # Si tienen distinto largo\n",
        "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
        "\n",
        "    # Predicción softmax\n",
        "    y_hat = np.argmax(model.predict(encoded)[0,-1,:])\n",
        "\n",
        "\n",
        "    # Debemos buscar en el vocabulario el caracter\n",
        "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "    out_word = ''\n",
        "    out_word = idx2char[y_hat]\n",
        "\n",
        "    # Agrego la palabra a la frase predicha\n",
        "    return human_text + out_word\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCeMWWupxN1-"
      },
      "source": [
        "### Generación de secuencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bwbS_pfhxvB3"
      },
      "outputs": [],
      "source": [
        "def generate_seq(model, seed_text, max_length, n_words):\n",
        "    \"\"\"\n",
        "        Exec model sequence prediction\n",
        "\n",
        "        Args:\n",
        "            model (keras): modelo entrenado\n",
        "            seed_text (string): texto de entrada (input_seq)\n",
        "            max_length (int): máxima longitud de la sequencia de entrada\n",
        "            n_words (int): números de caracteres a agregar a la sequencia de entrada\n",
        "        returns:\n",
        "            output_text (string): sentencia con las \"n_words\" agregadas\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "\t\t# Encodeamos\n",
        "        encoded = [char2idx[ch] for ch in output_text.lower() ]\n",
        "\t\t# Si tienen distinto largo\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "\t\t# Predicción softmax\n",
        "        y_hat = np.argmax(model.predict(encoded,verbose=0)[0,-1,:])\n",
        "\t\t# Vamos concatenando las predicciones\n",
        "        out_word = ''\n",
        "\n",
        "        out_word = idx2char[y_hat]\n",
        "\n",
        "\t\t# Agrego las palabras a la frase predicha\n",
        "        output_text += out_word\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Se efectúan pruebas con diversos textos de longitud variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JoFqRC5pxzqS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'no lo consentirían el duque de la milicia de la mil'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#input_text='como ya hemos dicho, no se h'\n",
        "#input_text='conservó sin mucho trabajo lo que tanto le h'\n",
        "#input_text='conservó sin mucho t'\n",
        "#input_text='no lo consentirían el duque de Milán ni los '\n",
        "input_text='no lo consentirían el duque de '\n",
        "generate_seq(model, input_text, max_length=max_context_size, n_words=20)\n",
        "#generate_seq(model, input_text, max_length=max_context_size, n_words=10)\n",
        "#generate_seq(model, input_text, max_length=max_context_size, n_words=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4kU-b2yeF-m"
      },
      "source": [
        "### Observación:\n",
        "\n",
        "- Modelo RNN: Se efectuaron pruebas con diversas partes literales del texto para verificar el comportamiento del modelo. Se encontraron muchos problemas, aunque si bien el texto conformado pareciera conservar cierta lógica sintáctica, no se verifica el mismo resultado evaluando la semántica. Es posible que para un modelo simple implementando tokenización por caracter, las relaciones semánticas no sean capturadas con la misma efectividad que para la sintaxis, donde la combinación de caracteres es lo relevante. Adicionalmente, se probó para diversas n_words, obteniendo resultados similares.\n",
        "\n",
        "- Modelo LSTM: Se repitieron pruebas respecto al modelo RNN, pero no se observaron grandes diferencias en cuanto a la efectividad del modelo para estimar siguientes letras/palabras. Se probó con diferentes largos de contexto, pero no se verificó un mejor comportamiento. Los altos tiempos de cómputo inherentes al procesamiento, aún con GPU, en Colab resultaron ser un problema en la dinámica de prueba/error. \n",
        "\n",
        "- Modelo GRU: Si bien el tiempo de entrenamiento se redujo lo cual facilitó la dinámica del trabajo, se repitieron pruebas pero no se observaron grandes diferencias en cuanto a la efectividad del modelo para estimar las siguientes letras/palabras de la secuencia. Se probó con diferentes largos de contexto, pero no se verificó un mejor comportamiento.\n",
        "\n",
        "- Se efectuaron pruebas con cadenas de términos obtenidas directamente desde el texto de entrenamiento y no se lograron mejoras significativas en la calidad de la predicción. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drJ6xn5qW1Hl"
      },
      "source": [
        "###  Beam search y muestreo aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_vovn9XZW1Hl"
      },
      "outputs": [],
      "source": [
        "# funcionalidades para hacer encoding y decoding\n",
        "\n",
        "def encode(text,max_length=max_context_size):\n",
        "\n",
        "    encoded = [char2idx[ch] for ch in text]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def decode(seq):\n",
        "    return ''.join([idx2char[ch] for ch in seq])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "I_lZiQwkW1Hl"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "# función que selecciona candidatos para el beam search\n",
        "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp,mode):\n",
        "\n",
        "  # colectar todas las probabilidades para la siguiente búsqueda\n",
        "  pred_large = []\n",
        "\n",
        "  for idx,pp in enumerate(pred):\n",
        "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
        "\n",
        "  pred_large = np.array(pred_large)\n",
        "\n",
        "  # criterio de selección\n",
        "  if mode == 'det':\n",
        "    idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
        "  elif mode == 'sto':\n",
        "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo aleatorio\n",
        "  else:\n",
        "    raise ValueError(f'Wrong selection mode. {mode} was given. det and sto are supported.')\n",
        "\n",
        "  # traducir a índices de token en el vocabulario\n",
        "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
        "                        np.array([idx_select%vocab_size]).T),\n",
        "                      axis=1)\n",
        "\n",
        "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
        "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
        "\n",
        "\n",
        "def beam_search(model,num_beams,num_words,input,temp=1,mode='det'):\n",
        "\n",
        "    # first iteration\n",
        "\n",
        "    # encode\n",
        "    encoded = encode(input)\n",
        "\n",
        "    # first prediction\n",
        "    y_hat = model.predict(encoded,verbose=0)[0,-1,:]\n",
        "\n",
        "    # get vocabulary size\n",
        "    vocab_size = y_hat.shape[0]\n",
        "\n",
        "    # initialize history\n",
        "    history_probs = [0]*num_beams\n",
        "    history_tokens = [encoded[0]]*num_beams\n",
        "\n",
        "    # select num_beams candidates\n",
        "    history_probs, history_tokens = select_candidates([y_hat],\n",
        "                                        num_beams,\n",
        "                                        vocab_size,\n",
        "                                        history_probs,\n",
        "                                        history_tokens,\n",
        "                                        temp,\n",
        "                                        mode)\n",
        "\n",
        "    # beam search loop\n",
        "    for i in range(num_words-1):\n",
        "\n",
        "      preds = []\n",
        "\n",
        "      for hist in history_tokens:\n",
        "\n",
        "        # actualizar secuencia de tokens\n",
        "        input_update = np.array([hist[i+1:]]).copy()\n",
        "\n",
        "        # predicción\n",
        "        y_hat = model.predict(input_update,verbose=0)[0,-1,:]\n",
        "\n",
        "        preds.append(y_hat)\n",
        "\n",
        "      history_probs, history_tokens = select_candidates(preds,\n",
        "                                                        num_beams,\n",
        "                                                        vocab_size,\n",
        "                                                        history_probs,\n",
        "                                                        history_tokens,\n",
        "                                                        temp,\n",
        "                                                        mode)\n",
        "\n",
        "    return history_tokens[:,-(len(input)+num_words):]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementación y pruebas con Beam Search\n",
        "\n",
        "- Mediante la implementación de éste algoritmo se pretede seleccionar las mejores secuencias candidatas a partir de las predicciones actuales, considerando la historia de tokens y probabilidades acumuladas.\n",
        "- Se efectuaron pruebas para el modo determinístico y para estócastico. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "GeLqAoOYW1Hm"
      },
      "outputs": [],
      "source": [
        "# predicción con beam search\n",
        "#salidas = beam_search(model,num_beams=10,num_words=20,input=\"conservó sin mucho trabajo lo q\", mode='sto')\n",
        "salidas = beam_search(model,num_beams=10,num_words=20,input=\"conservó sin mucho tr\", mode='sto')\n",
        "#salidas = beam_search(model,num_beams=10,num_words=20,input=\"conservó sin mucho trabajo lo q\", mode='det')\n",
        "#salidas = beam_search(model,num_beams=5,num_words=20,input=\"conservó sin mucho trabajo lo q\", mode='sto')\n",
        "#salidas = beam_search(model,num_beams=5,num_words=20,input=\"conservó sin mucho trabajo lo q\", mode='det')\n",
        "#salidas = beam_search(model,num_beams=10,num_words=10,input=\"conservó sin mucho trabajo lo q\", mode='sto')\n",
        "#salidas = beam_search(model,num_beams=10,num_words=10,input=\"habia una vez\", mode='det')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "P8HQoLhw-NYg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([27, 32, 48,  3, 24, 49,  9, 53, 51,  3, 13, 48, 51, 54, 12, 27, 22,\n",
              "       32, 51,  5, 49,  4, 38,  4,  8, 32, 51, 33, 24, 51, 11,  4, 51, 17,\n",
              "       32, 49,  5, 12, 48,  4,  2])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "salidas[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "2S3_I3S1W1Hm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'conservó sin mucho trabajo de la fortuna,'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# veamos las salidas\n",
        "decode(salidas[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CEIA-IA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
