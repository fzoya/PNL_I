{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZd5yLnnHOK0"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "- Alumno: Federico M. Zoya\n",
    "\n",
    "## Custom embedddings con Gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA7nqkumo9z9"
   },
   "source": [
    "### Objetivo\n",
    "El objetivo es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto. Se utilizará canciones de bandas para generar los embeddings, es decir, que los vectores tendrán la forma en función de como esa banda haya utilizado las palabras en sus canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lFToQs5FK5uZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g07zJxG7H9vG"
   },
   "source": [
    "### Datos\n",
    "Utilizaremos como dataset La Biblia descargada en formato texto plano de https://archive.org/stream/la-biblia-latinoamericana_202308/La%20Biblia%20Latinoamericana_djvu.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ticoqYD1Z3I7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¡Jesús ha resucitado!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ustedes que abren la Biblia, busquen a Jesús. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>para rezar, o para instrucción nuestra, La Bib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>la vida.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>En el centro de la Biblia está la Cruz de Jesú...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                              ¡Jesús ha resucitado!\n",
       "1  Ustedes que abren la Biblia, busquen a Jesús. ...\n",
       "2  para rezar, o para instrucción nuestra, La Bib...\n",
       "3                                           la vida.\n",
       "4  En el centro de la Biblia está la Cruz de Jesú..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Armar el dataset utilizando salto de línea para separar las oraciones/docs\n",
    "df = pd.read_csv('La Biblia.txt', sep='/n', header=None, engine='python')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LEpKubK9XzXN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de documentos: 178768\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de documentos:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab94qaFlrA1G"
   },
   "source": [
    "### 1 - Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rIsmMWmjrDHd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 15:26:02.457156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747938362.645187   36427 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747938362.698867   36427 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747938363.155330   36427 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747938363.155361   36427 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747938363.155363   36427 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747938363.155364   36427 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-22 15:26:03.205600: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#from keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence_tokens = []\n",
    "# Recorrer todas las filas y transformar las oraciones\n",
    "# en una secuencia de palabras (esto podría realizarse con NLTK o spaCy también)\n",
    "for _, row in df[:None].iterrows():\n",
    "    sentence_tokens.append(text_to_word_sequence(str(row[0]))) #Se fuerza conversión a string debido a errores obtenidos por datos interpretados como flontantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se parsea el texto, es decir, se procesa cada documento que conforma el corpus, y se separan en palabras que luego serán vectorizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CHepi_DGrbhq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['¡jesús', 'ha', 'resucitado'],\n",
       " ['ustedes',\n",
       "  'que',\n",
       "  'abren',\n",
       "  'la',\n",
       "  'biblia',\n",
       "  'busquen',\n",
       "  'a',\n",
       "  'jesús',\n",
       "  'la',\n",
       "  'biblia',\n",
       "  'no',\n",
       "  'es',\n",
       "  'un',\n",
       "  'libro',\n",
       "  'solamente'],\n",
       " ['para',\n",
       "  'rezar',\n",
       "  'o',\n",
       "  'para',\n",
       "  'instrucción',\n",
       "  'nuestra',\n",
       "  'la',\n",
       "  'biblia',\n",
       "  'es',\n",
       "  'palabra',\n",
       "  'de',\n",
       "  'dios',\n",
       "  'para',\n",
       "  'comunicarnos']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demos un vistazo\n",
    "sentence_tokens[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaXV6nlHr5Aa"
   },
   "source": [
    "### 2 - Crear los vectores (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OSb0v7h8r7hK"
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
    "# Sobrecargamos el callback para poder tener esta información\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss - self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se sobrecarga el método callback para visualizar el error por cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "i0wnDdv9sJ47"
   },
   "outputs": [],
   "source": [
    "# Crearmos el modelo generador de vectores\n",
    "# En este caso utilizaremos la estructura modelo Skipgram\n",
    "w2v_model = Word2Vec(min_count=5,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
    "                     window=2,       # cant de palabras antes y desp de la predicha\n",
    "                     #vector_size=300,       # dimensionalidad de los vectores \n",
    "                     #vector_size=15,       # Como el vocabulario ronda las 14 mil palabras => Dim{Embeddings} = Raiz Cuarta de 14 mil => Aprox 11. Se define 15 a modo de prueba.\n",
    "                     vector_size=100, \n",
    "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
    "                     workers=1,      # si tienen más cores pueden cambiar este valor\n",
    "                     sg=1)           # modelo 0:CBOW  1:skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se instancia el vectorizados con los parámetros requeridos, se efectúan pruebas definiendo distintos largos de embeddings para corroborar resultados. En este sentido se verifica que la dimensión del vector afecta el resultado de la vectorización, y por consecuencia, del resultado de las operaciones vectoriales relacionadas a las mètricas de interes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5lTt8wErsf17"
   },
   "outputs": [],
   "source": [
    "# Obtener el vocabulario con los tokens\n",
    "w2v_model.build_vocab(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A partir de la instancia del modelo y los términos del corpus, se genera el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TNc9qt4os5AT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de docs en el corpus: 178768\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de filas/docs encontradas en el corpus\n",
    "print(\"Cantidad de docs en el corpus:\", w2v_model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "idw9cHF3tSMl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de words distintas en el corpus: 14423\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de words encontradas en el corpus\n",
    "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC9mZ8DPk-UC"
   },
   "source": [
    "### 3 - Entrenar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QSp-x0PAsq56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 6722672.0\n",
      "Loss after epoch 1: 5065860.0\n",
      "Loss after epoch 2: 4909357.0\n",
      "Loss after epoch 3: 4686247.0\n",
      "Loss after epoch 4: 4598276.0\n",
      "Loss after epoch 5: 4525242.0\n",
      "Loss after epoch 6: 4520630.0\n",
      "Loss after epoch 7: 4536752.0\n",
      "Loss after epoch 8: 4471308.0\n",
      "Loss after epoch 9: 4410196.0\n",
      "Loss after epoch 10: 4366448.0\n",
      "Loss after epoch 11: 4324472.0\n",
      "Loss after epoch 12: 4295224.0\n",
      "Loss after epoch 13: 4265420.0\n",
      "Loss after epoch 14: 2048216.0\n",
      "Loss after epoch 15: 964512.0\n",
      "Loss after epoch 16: 951952.0\n",
      "Loss after epoch 17: 937768.0\n",
      "Loss after epoch 18: 928064.0\n",
      "Loss after epoch 19: 922016.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17531623, 25882540)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo generador de vectores\n",
    "# Utilizamos nuestro callback\n",
    "w2v_model.train(sentence_tokens,\n",
    "                 total_examples=w2v_model.corpus_count,\n",
    "                 epochs=20,\n",
    "                 compute_loss = True,\n",
    "                 callbacks=[callback()]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se entrena el modelo para que genere un espacio de vectores denso a partir de los tokens generados con las palabras del vocabulario. Esto permite disponer de un espacio donde cada vector representa un concepto inferido a partir de lo aprendido en la etapa de entrenamiento. Es de esperar que la dirección de cada vector se correlacione semánticamente con los conceptos que el vector representa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddT9NVuNlCAe"
   },
   "source": [
    "### 4 - Ensayar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6cHN9xGLuPEm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pablo', 0.6132212281227112),\n",
       " ('pedro', 0.6110264658927917),\n",
       " ('pilato', 0.5882518291473389),\n",
       " ('juan', 0.5843722224235535),\n",
       " ('ezequiel', 0.5701648592948914),\n",
       " ('vic', 0.5612123608589172),\n",
       " ('endemoniado', 0.5575493574142456),\n",
       " ('lutero', 0.5504878163337708),\n",
       " ('resucitado', 0.5475106239318848),\n",
       " ('canso', 0.5423581004142761)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"jesús\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A partir de un nombre (jesús en este caso), se obtienen en el listado de palabras similares otros nombres propios mencionados en el corpus. Es en este caso la asociación semántica predominante resulta evidente en las primeras palabras del listado. Tambien aparecen otras palabras que no son nombres propios, pero que guardan relación menos directa con la palabra buscada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "47HiU5gdkdMq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('torres', 0.05908196046948433),\n",
       " ('for', 0.046074312180280685),\n",
       " ('maría', 0.0320778489112854),\n",
       " ('hebrón', 0.02323792688548565),\n",
       " ('ciudades', 0.019428109750151634),\n",
       " ('tiendas', 0.013767692260444164),\n",
       " ('segunda', 0.013088934123516083),\n",
       " ('reúne', 0.009724117815494537),\n",
       " ('587', 0.009182213805615902),\n",
       " ('reinó', 0.00846576876938343)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MENOS se relacionan con...:\n",
    "#w2v_model.wv.most_similar(negative=[\"biblia\"], topn=10)\n",
    "#w2v_model.wv.most_similar(negative=[\"jesús\"], topn=10)\n",
    "w2v_model.wv.most_similar(negative=[\"malo\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Listado de palabras que menos se parecen o que menos correlacionadas están, es decir vectores asociados a una mayor ortogonalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DT4Rvno2mD65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('darle', 0.6985543966293335),\n",
       " ('darlos', 0.6460245847702026),\n",
       " ('darles', 0.6194551587104797),\n",
       " ('darme', 0.6106230020523071),\n",
       " ('intervenir', 0.6046268939971924),\n",
       " ('darse', 0.6012704968452454),\n",
       " ('dárselo', 0.5958026647567749),\n",
       " ('recuperar', 0.5894939303398132),\n",
       " ('darnos', 0.5840307474136353),\n",
       " ('lograr', 0.5818242430686951)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"dar\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XPLDPgzBmQXt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('misericordia', 0.7392441034317017),\n",
       " ('ternura', 0.6442787647247314),\n",
       " (\"a'la\", 0.6435837149620056),\n",
       " ('lucidez', 0.6393976211547852),\n",
       " ('paciente', 0.6339818239212036)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"bondad\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se verifica con diversas pruebas la relación existente entre las palabras elegidas para la prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "L_UvHPMMklOr"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'salamín' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ensayar con una palabra que no está en el vocabulario:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mw2v_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnegative\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msalamín\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClasesIA/CEIA-IA/lib/python3.12/site-packages/gensim/models/keyedvectors.py:841\u001b[39m, in \u001b[36mKeyedVectors.most_similar\u001b[39m\u001b[34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[39m\n\u001b[32m    838\u001b[39m         weight[idx] = item[\u001b[32m1\u001b[39m]\n\u001b[32m    840\u001b[39m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m mean = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m all_keys = [\n\u001b[32m    843\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_index_for(key)\n\u001b[32m    844\u001b[39m ]\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClasesIA/CEIA-IA/lib/python3.12/site-packages/gensim/models/keyedvectors.py:518\u001b[39m, in \u001b[36mKeyedVectors.get_mean_vector\u001b[39m\u001b[34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[39m\n\u001b[32m    516\u001b[39m         total_weight += \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not present in vocabulary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total_weight > \u001b[32m0\u001b[39m:\n\u001b[32m    521\u001b[39m     mean = mean / total_weight\n",
      "\u001b[31mKeyError\u001b[39m: \"Key 'salamín' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# Ensayar con una palabra que no está en el vocabulario:\n",
    "w2v_model.wv.most_similar(negative=[\"salamín\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27948487  0.5422419   0.01192401 -0.3974666   0.38953003 -0.48999876\n",
      "  0.38978606  0.543595    0.31481418 -0.3885466   0.33346367 -0.42916143\n",
      " -0.2023289  -0.675008    0.07852904  0.12250926 -0.47225922 -0.42087248\n",
      "  0.7170898  -1.211247    0.5119354   0.10889079  1.1489388   0.33706596\n",
      " -0.11894103  0.32769978  0.05467651 -0.2320251  -0.08386101 -0.01251764\n",
      "  0.02802159  0.19031079  0.17165548 -0.89250994 -0.7527319   0.3350974\n",
      " -0.14664821  0.9127899  -0.18524483 -0.99529576  0.4881246  -0.21661639\n",
      "  0.5143047   0.2038148   0.17546597 -0.22342023 -0.021809   -0.35668015\n",
      "  0.13262922  0.18567744  0.01088025 -0.40845698 -0.32363385 -0.03253867\n",
      "  0.00858937  0.01135063 -0.74698657 -0.20404924  0.02675049 -0.06095853\n",
      "  0.06006893 -0.08676057  0.07409714 -0.1152029  -0.47674668  0.0422935\n",
      "  0.13947846  0.25103992 -0.3871872   0.77008545  0.4083684  -0.03415367\n",
      "  0.9184711   0.13794102  0.2647675   0.45352966 -0.01044037 -0.6211611\n",
      "  0.0694292  -0.2806563   0.4804734  -0.31514102 -0.07220432  0.2353895\n",
      " -0.2648227   0.20694333 -0.41111168  0.17601699 -0.66833174  0.4534699\n",
      "  0.23121345  0.4669027  -0.07710458  0.3963604   0.3845424   1.3024497\n",
      " -0.33189133  0.26907063  0.17554918  0.54449713]\n"
     ]
    }
   ],
   "source": [
    "# el método `get_vector` permite obtener los vectores:\n",
    "vector_dios= w2v_model.wv.get_vector(\"bondad\")\n",
    "print(vector_dios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se visualizan los valores del embedding asociado al término \"bondad\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bondad', 1.0000001192092896),\n",
       " ('misericordia', 0.7392441034317017),\n",
       " ('ternura', 0.6442787647247314),\n",
       " (\"a'la\", 0.6435837149620056),\n",
       " ('lucidez', 0.6393976211547852),\n",
       " ('paciente', 0.6339818239212036),\n",
       " ('fide', 0.6302697062492371),\n",
       " ('fideli', 0.6267864108085632),\n",
       " ('sálvanos', 0.6266052722930908),\n",
       " ('benevolencia', 0.6225915551185608)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# el método `most_similar` también permite comparar a partir de vectores\n",
    "w2v_model.wv.most_similar(vector_dios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('misericordia', 0.5888675451278687),\n",
       " ('designio', 0.5852081179618835),\n",
       " ('anhelo', 0.583711564540863),\n",
       " ('dinamismo', 0.5808125138282776),\n",
       " ('apasionado', 0.5769360661506653),\n",
       " ('tierno', 0.5712573528289795),\n",
       " ('infinita', 0.5691033005714417),\n",
       " ('fraterno', 0.5668566823005676),\n",
       " ('misericordioso', 0.5591129660606384),\n",
       " ('porvenir', 0.5586978793144226)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"amor\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g8UVWe6lFmh"
   },
   "source": [
    "### 5 - Visualizar agrupación de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "pDxEVXAivjr9"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    \n",
    "from sklearn.manifold import TSNE                   \n",
    "import numpy as np                                  \n",
    "\n",
    "def reduce_dimensions(model, num_dimensions = 2 ):\n",
    "     \n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  \n",
    "\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    return vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "NCCXtDpcugmd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecs: <class 'numpy.ndarray'> (14423, 2)\n",
      "Labels: <class 'numpy.ndarray'> 14423\n",
      "NaNs in vecs: False\n"
     ]
    }
   ],
   "source": [
    "# Graficar los embedddings en 2D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_model)\n",
    "\n",
    "MAX_WORDS=200\n",
    "fig = px.scatter(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], text=labels[:MAX_WORDS])\n",
    "#fig.show(renderer=\"colab\") # esto para plotly en colab\n",
    "print(\"Vecs:\", type(vecs), vecs.shape if hasattr(vecs, \"shape\") else \"No shape\")\n",
    "print(\"Labels:\", type(labels), len(labels) if hasattr(labels, \"__len__\") else \"No length\")\n",
    "print(\"NaNs in vecs:\", np.isnan(vecs).any())\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los embedddings en 3D\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_model,3)\n",
    "\n",
    "fig = px.scatter_3d(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], z=vecs[:MAX_WORDS,2],text=labels[:MAX_WORDS])\n",
    "fig.update_traces(marker_size = 2)\n",
    "#fig.show(renderer=\"colab\") # esto para plotly en colab\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es posible verificar gráficamente las relaciones semánticas entre palabras por su cercanía en el espacio de embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones:\n",
    "\n",
    "- En el presente trabajo se pudieron verificar conceptos presentados en clase a partir del procesamiento de un texto cualquiera, en este caso, uno correspondiente a La biblia. Mediante el paquete Gensim se implementó un modelo word2vec para la vectorización de los tokens generados a partir del corpus procesado. Una vez entrenado el modelo y generado el espacio de embeddings, se procedió a verificar las métricas eligiendo palabras al azar del corpus y comprobando la similitud con otras palabras generando listados top10, tanto para las diez mas parecidas como para las menos parecidas. Por último se generaron representaciones gráficas en 2D y 3D del espacio generado utilizando T-SNE, con el objetivo de visualizar un manifold representativo de dicho espacio.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CEIA-IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
